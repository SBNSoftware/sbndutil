[global]
group      = sbnd
experiment = sbnd
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
version = v07_07_00_2_MCP0_9
quals   = e17:prof
#neventsperjob needs to be -1 for downstream stages to process every event in the file 
neventsperjob = -1 
fclfile = override_me
mdprojectname = override_me
mdprojectstage = override_me
#This hacky naming scheme is so we can use a different production name for test jobs easily in fife_wrap poms
productionlabel = TP0.5
mdproductionname = %(productionlabel)s
#This name is used to identify the tet jobs submitted as part of a production
testmdproductionname = test_%(productionlabel)s
mdproductiontype = test
basename= stage_override
outputdatasetname=%(mdproductiontype)s_%(mdproductionname)s_%(mdprojectname)s_%(mdprojectstage)s
testoutputdatasetname=%(mdproductiontype)s_%(testmdproductionname)s_%(mdprojectname)s_%(mdprojectstage)s
baseoutdir=/pnfs/sbnd/scratch/sbndpro/dropbox/mc/%(mdproductiontype)s/%(mdproductionname)s/%(version)s/%(mdprojectname)s
#this dir is for output of test jobs to avoid writing too much to tape.  It's monitored by the FTS but copies to another scratch dcache location
testoutdir=/pnfs/sbnd/scratch/sbndpro/dropbox/pomstestjobs/mc/origin/%(mdproductiontype)s/%(mdproductionname)s/%(version)s/%(mdprojectname)s/%(basename)s 
#19/07 DBrailsford
#Disable sam_dataset as we let POMS use %(dataset)s in the POMS interface.  This is automatically setup and used provided all of the dependent POMS stages are set up prior to launching the campaign
#sam_dataset = override_me
streamname = only
#06/11/18 DBrailsford
#Use jobname with fife_utils 3_2_4 to override the name of the submitted job.  It makes book-keeping a bit easier
jobname=%(mdproductionname)s__%(mdprojectstage)s__%(mdprojectname)s
[env_pass]
IFDH_DEBUG = 1
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s
[submit]
G          = %(group)s
N          = '`samweb -e %(experiment)s list-definition-files %(dataset)s | wc -l`'
#N          = 'os.system(\"samweb -e %(experiment)s list-definition-files %(dataset)s | wc -l\")'
#dataset     =
#resource-provides      = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
resource-provides      = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
generate-email-summary = True
#expected-lifetime      = 3h
#timeout                      = 2h
OS                     = SL6
##disk                  = 10GB
#memory                 = 2000MB
#f_1                    = /pnfs/sbnd/scratch/sbndpro/dbrailsf_poms_test/prescripts/sbndpoms_tfilemetadata_extractor.sh
#f_2                    = /pnfs/sbnd/scratch/sbndpro/dbrailsf_poms_test/prescripts/sbndpoms_metadata_injector.sh
#f_3                    = /pnfs/sbnd/scratch/sbndpro/dbrailsf_poms_test/postscript/sbndpoms_ifdh_mkdir-p.sh

[job_setup]
debug       = True
find_setups = True
source_1    = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1     = %(experiment)scode %(version)s -q %(quals)s

prescript_1 = sbndpoms_wrapperfcl_maker.sh --fclname %(fclfile)s --wrappername wrapper.fcl
prescript_2 = sbndpoms_metadata_injector.sh --inputfclname wrapper.fcl --mdfclname %(fclfile)s --mdprojectname %(mdprojectname)s --mdprojectstage %(mdprojectstage)s --mdprojectversion %(version)s --mdprojectsoftware sbndcode --mdproductionname %(mdproductionname)s --mdproductiontype %(mdproductiontype)s --mdappversion %(version)s --mdfiletype mc --mdappfamily art --mdruntype physics --tfilemdjsonname hist_%(basename)s.root.json
#prescript_3 = chmod +x ${CONDOR_DIR_INPUT}/sbndpoms_tfilemetadata_extractor.sh
prescript_3 = cat wrapper.fcl
postscript_1 = ls
postscript_2 = sbndpoms_metadata_extractor.sh %(basename)s.root
postscript_3 = sbndpoms_tfilemetadata_extractor.sh hist_%(basename)s.root

multifile   = True
#multifile   = True
#getconfig   = False
#ifdh_art    = False
[sam_consumer]
limit       = 1
appvers     = %(version)s
schema      = root
[executable]
name       = lar
arg_1      = -c
arg_2      = wrapper.fcl
arg_3      = -o
arg_4      = %(basename)s.root
arg_5      = -T 
arg_6      = hist_%(basename)s.root
arg_7      = -s 
#arg_8      = input_filename -- will be added by multifile loop...
[job_output]
addoutput   = %(basename)s.root
rename      = unique
dest        = %(baseoutdir)s
declare_metadata = True
metadata_extractor=sbndpoms_metadata_extractor.sh
add_location=False 
add_to_dataset  = %(outputdatasetname)s
dataset_exclude = hist*

[job_output_1]
addoutput   = hist_%(basename)s.root
rename      = unique
dest        = %(baseoutdir)s
declare_metadata = True
#metadata_extractor=${CONDOR_DIR_INPUT}/sbndpoms_tfilemetadata_extractor.sh --inputfile %s --protojson hist_gen.root.json
metadata_extractor=sbndpoms_tfilemetadata_extractor.sh
add_location=False 

[stage_gen]
job_setup.prescript_4 = sbndpoms_runnumber_injector.sh --fcl wrapper.fcl 
job_setup.prescript_5 = cat wrapper.fcl
# fake output dataset for POMS
#For the initial stage which doesn't use an input dataset, the output dataset needs to be labelled according to a standard expected by POMS
job_output.add_to_dataset  = _poms_task
job_output.dataset_exclude = hist*
#make the additional dataset to the sbnd standard.  Currently does not work
job_setup.postscript_4=ifdh createDefinition %(outputdatasetname)s defname:poms_depends_${POMS_TASK_ID}_1 ${GRID_USER} ${EXPERIMENT}
job_setup.postscript_5=ifdh createDefinition poms_depends_${POMS_TASK_ID}_2 defname:poms_depends_${POMS_TASK_ID}_1 ${GRID_USER} ${EXPERIMENT}
job_setup.postscript_6=ifdh createDefinition poms_depends_${POMS_TASK_ID}_3 defname:poms_depends_${POMS_TASK_ID}_1 ${GRID_USER} ${EXPERIMENT}
# turn off -s flag
executable.arg_7           =  
#02/10 DBrailsford: tell the generator stage how many events in each job we want 
#arg_7 is not a typo
executable.arg_7           = -n 
executable.arg_8           = %(neventsperjob)s
global.basename            = gen
global.mdprojectstage      = %(basename)s
job_setup.multifile        = False
job_output.dest            = %(baseoutdir)s/%(basename)s
job_output_1.dest          = %(baseoutdir)s/%(basename)s
[stage_g4]
global.fclfile        = standard_g4_%(experiment)s.fcl
global.basename       = g4
global.mdprojectstage = g4
submit.dataset        = %(dataset)s
job_output.dest       = %(baseoutdir)s/g4
job_output_1.dest     = %(baseoutdir)s/g4
# # if g4 only works onsite 
submit.resource-provides= usage_model=OPPORTUNISTIC,DEDICATED
#
# # ...with extra cvmfs libraries:
# job_setup.prescript     = export LD_LIBRARY_PATH=/cvmfs/nova.opensciencegrid.org/externals/library_shim/v03.03/NULL/lib/sl6:$LD_LIBRARY_PATH
[stage_detsim]
global.basename       = detsim
global.mdprojectstage = detsim
global.fclfile        = standard_detsim_%(experiment)s.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(baseoutdir)s/detsim
job_output_1.dest     = %(baseoutdir)s/detsim
[stage_reco]
global.basename       = reco
global.mdprojectstage = reco
global.fclfile        = standard_reco_%(experiment)s_basic.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(baseoutdir)s/reco
job_output_1.dest     = %(baseoutdir)s/reco
[stage_anatree]
global.basename       = anatree
global.mdprojectstage = anatree
global.fclfile        = standard_anatree_%(experiment)s.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(baseoutdir)s/anatree
job_output_1.dest     = %(baseoutdir)s/anatree
#Disable the AROTROOT output
executable.arg_3      = 
executable.arg_4      = 
executable.arg_6      = %(basename)s.root
#19/07 DBrailsford
#This is a bit hacky right now.  I've asked marc about passing arguments to the metadata_extractor which would mean this isn't necessary.
job_setup.prescript_2 = sbndpoms_metadata_injector.sh --inputfclname wrapper.fcl --mdfclname %(fclfile)s --mdprojectname %(mdprojectname)s --mdprojectstage %(mdprojectstage)s --mdprojectversion %(version)s --mdprojectsoftware sbndcode --mdproductionname %(mdproductionname)s --mdproductiontype %(mdproductiontype)s --mdappversion %(version)s --mdfiletype mc --mdappfamily art --mdruntype physics --tfilemdjsonname %(basename)s.root.json
#Point job_output towards a non-existent output as it isn't used for this stage
job_output.addoutput   = no_output.root
job_output_1.addoutput = %(basename)s.root
job_output_1.add_to_dataset  = %(outputdatasetname)s

[stage_split]
global.basename = split
arg_3           =
arg_4           =
global.fclfile  = standard_streamsplit_%(experiment)s.fcl
submit.dataset  = %(dataset)s
[stage_merge]
global.basename = merge_%(streamname)s
global.fclfile  = standard_merge_%(experiment)s.fcl
submit.dataset  = %(dataset)s


