[global]
group      = sbnd
experiment = sbnd
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
version = v09_14_00
quals   = e19:prof
#neventsperjob needs to be -1 for downstream stages to process every event in the file
neventsperjob = -1
fclfile = override_me
mdprojectname = override_me
mdprojectstage = override_me
#This hacky naming scheme is so we can use a different production name for test jobs easily in fife_wrap poms
productionlabel = TP4.1
mdproductionname = %(productionlabel)s
#This name is used to identify the tet jobs submitted as part of a production
testmdproductionname = test_%(productionlabel)s
mdproductiontype = test
basename= stage_override
#02/10/21 mateusc
#The following dirs used to be monitored by FTS and copied to another scratch dcache location. This configuration remains only for tapeoutdir.
#The remaining dirs are the final output dirs were they will be declared to SAM.
outputdatasetname=%(mdproductiontype)s_%(mdproductionname)s_%(mdprojectname)s_%(mdprojectstage)s_sbnd
testoutputdatasetname=%(mdproductiontype)s_%(testmdproductionname)s_%(mdprojectname)s_%(mdprojectstage)s_sbnd
tapeoutdir=/pnfs/sbnd/scratch/sbndpro/dropbox/mc_totape/%(mdproductiontype)s/%(mdproductionname)s/%(version)s/%(mdprojectname)s/%(basename)s
scratchoutdir=/pnfs/sbnd/scratch/sbndpro/dropbox/mc_toscratch/source/%(mdproductiontype)s/%(mdproductionname)s/%(version)s/%(mdprojectname)s/%(basename)s
testoutdir=/pnfs/sbnd/scratch/sbndpro/dropbox/pomstestjobs/mc/origin/%(mdproductiontype)s/%(mdproductionname)s/%(version)s/%(mdprojectname)s/%(basename)s
streamname = only
#06/11/18 DBrailsford
#Use jobname with fife_utils 3_2_4 to override the name of the submitted job.  It makes book-keeping a bit easier
jobname=%(mdproductionname)s__%(mdprojectstage)s__%(mdprojectname)s_
[env_pass]
IFDH_DEBUG = 1
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s
[submit]
G          = %(group)s
#12/02/19 DBrailsford
#Use n_files_per_job to define how many jobs to run as suggested by Mengel
n_files_per_job        = 1
#resource-provides      = usage_model=DEDICATED,OPPORTUNISTIC
resource-provides      = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
generate-email-summary = True
#expected-lifetime      = 3h
#timeout                      = 2h
#21/11/19 DBrailsford
#Require only SL7 to as the FNAL nodes don't like no specific request
#OS                     = SL6,SL7
OS                     = SL7
##disk                  = 10GB
#memory                 = 2000MB
#05/11/19 DBrailsford
#Use SL7 singularity container
line_1 = '+SingularityImage=\"/cvmfs/singularity.opensciencegrid.org/fermilab/fnal-wn-sl7:latest\"'
#13/02/19 DBrailsford
#Don't restart a job if it is evicted, let it count as a failure and let recovery pick up the slack
# 26/10/20 ETyley
# Newer version of SAM should handle this okay now. Remove to add the autorecovery of held jobs
#line_2 = '+PeriodicRemove=(NumJobStarts>=1&&JobStatus==1)'
# 26/10/20 ETyley Autorecovery for held jobs with an extra grid slot and 2 hours of runtime
line_2 = '+FERMIHTC_AutoRelease=True'
line_3 = '+FERMIHTC_GraceMemory=2048'
line_4 = '+FERMIHTC_GraceLifetime=7200'
#21/03/19 DBrailsford
#Only use sites which have stashcache access
#05/11/19 DBrailsford
#Require singularity
append_condor_requirements='(TARGET.HAS_CVMFS_sbnd_osgstorage_org==true)&&(TARGET.HAS_CVMFS_sbnd_opensciencegrid_org==true)&&(TARGET.HAS_SINGULARITY=?=true)'
[job_setup]
debug       = True
find_setups = True
source_1    = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1     = %(experiment)scode %(version)s -q %(quals)s
prescript_1 = sbndpoms_wrapperfcl_maker.sh --fclname %(fclfile)s --wrappername wrapper.fcl
prescript_2 = sbndpoms_metadata_injector.sh --inputfclname wrapper.fcl --mdfclname %(fclfile)s --mdprojectname %(mdprojectname)s --mdprojectstage %(mdprojectstage)s --mdprojectversion %(version)s --mdprojectsoftware sbndcode --mdproductionname %(mdproductionname)s --mdproductiontype %(mdproductiontype)s --mdappversion %(version)s --mdfiletype mc --mdappfamily art --mdruntype physics --tfilemdjsonname hist_%(basename)s.root.json --mdgroupname %(group)s
#prescript_3 = chmod +x ${CONDOR_DIR_INPUT}/sbndpoms_tfilemetadata_extractor.sh
#DIAGNOSTIC
prescript_3 = cat wrapper.fcl
#prescript_4 = ifdh cp -D /pnfs/sbnd/persistent/sbndpro/scripts/sbndpoms_metadata_extractor.sh $PWD
#prescript_5 = chmod +x sbndpoms_metadata_extractor.sh
#ALL postscripts here are diagnostics
postscript_1 = ls
postscript_2 = sbndpoms_metadata_extractor.sh %(basename)s.root
postscript_3 = sbndpoms_tfilemetadata_extractor.sh hist_%(basename)s.root
postscript_4 = echo "postscript_4 has just run"
multifile   = True
[sam_consumer]
limit       = 1
appvers     = %(version)s
schema      = root
[executable]
name       = lar
arg_1      = -c
arg_2      = wrapper.fcl
arg_3      = -o
arg_4      = %(basename)s.root
# 26/10/20 ETyley
# Disable hist files being returned by default
#arg_5      = -T
#arg_6      = hist_%(basename)s.root
arg_5      =
arg_6      =
arg_7      = -s
#arg_8      = input_filename -- will be added by multifile loop...

[job_output]
addoutput   = %(basename)s.root
rename      = unique
dest        = %(scratchoutdir)s
declare_metadata = True
metadata_extractor=sbndpoms_metadata_extractor.sh
add_location = True #02/10/21 mateusc
add_to_dataset  = %(outputdatasetname)s
dataset_exclude = hist*
hash = 2 #02/10/21 mateusc

[job_output_1]
addoutput   = hist_%(basename)s.root
rename      = unique
dest        = %(scratchoutdir)s
declare_metadata = True
metadata_extractor=sbndpoms_tfilemetadata_extractor.sh
add_location = True #02/10/21 mateusc

[job_output_2]
addoutput   = caf_%(basename)s.root
rename      = unique
dest        = %(scratchoutdir)s
declare_metadata = True
metadata_extractor=sbndpoms_tfilemetadata_extractor.sh
add_location = True #02/10/21 mateusc

[stage_gen]
global.basename       = gen
global.mdprojectstage = gen
submit.dataset        = %(dataset)s
#job_output.dest       = %(scratchoutdir)s/gen
#job_output_1.dest     = %(scratchoutdir)s/gen
executable.arg_1      = -o
executable.arg_2      = %(basename)s.root
# 26/10/20 ETyley
# Disable hist files being returned by default
#executable.arg_3      = -T
#executable.arg_4      = hist_%(basename)s.root
executable.arg_3      =
executable.arg_4      =
executable.arg_5      = -n
executable.arg_6      = %(neventsperjob)s
executable.arg_7      =
job_setup.multifile   = False
job_setup.getconfig   = True
#obviously diagnostics
job_setup.prescript_1 =  echo prescript_1 overriden in gen to do nothing
job_setup.prescript_2 =  echo prescript_2 overriden in gen to do nothing
job_setup.prescript_3 =  echo prescript_3 overriden in gen to do nothing

[stage_g4]
global.fclfile        = standard_g4_%(experiment)s.fcl
global.basename       = g4
global.mdprojectstage = g4
submit.dataset        = %(dataset)s
#job_output.dest       = %(scratchoutdir)s/g4
#job_output_1.dest     = %(scratchoutdir)s/g4
#17/02/19 DBrailsford: Load dune_oslibs so that we can run offsite
#04/11/19 DBrailsford: Singularity containers negate the need for the os library set
#job_setup.setup_2     = dune_oslibs v1_0_0 -z /cvmfs/dune.opensciencegrid.org/products/dune/

[stage_detsim]
global.basename       = detsim
global.mdprojectstage = detsim
global.fclfile        = standard_detsim_%(experiment)s.fcl
submit.dataset        = %(dataset)s
#job_output.dest       = %(scratchoutdir)s/detsim
#job_output_1.dest     = %(scratchoutdir)s/detsim
[stage_reco1]
global.basename       = reco1
global.mdprojectstage = reco1
global.fclfile        = standard_reco1_%(experiment)s_basic.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(tapeoutdir)s
#job_output_1.dest     = %(tapeoutdir)s
[stage_reco2]
global.basename       = reco2
global.mdprojectstage = reco2
global.fclfile        = standard_reco2_%(experiment)s_basic.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(tapeoutdir)s
job_output.add_location = False #02/10/21 mateusc
#job_output_1.dest     = %(tapeoutdir)s
[stage_anatree]
global.basename       = anatree
global.mdprojectstage = anatree
global.fclfile        = standard_anatree_%(experiment)s.fcl
submit.dataset        = %(dataset)s
job_output.dest       = %(tapeoutdir)s
#job_output_1.dest     = %(tapeoutdir)s
#Disable the AROTROOT output
executable.arg_3      =
executable.arg_4      =
executable.arg_6      = %(basename)s.root
#19/07 DBrailsford
#This is a bit hacky right now.  I've asked marc about passing arguments to the metadata_extractor which would mean this isn't necessary.
job_setup.prescript_2 = sbndpoms_metadata_injector.sh --inputfclname wrapper.fcl --mdfclname %(fclfile)s --mdprojectname %(mdprojectname)s --mdprojectstage %(mdprojectstage)s --mdprojectversion %(version)s --mdprojectsoftware sbndcode --mdproductionname %(mdproductionname)s --mdproductiontype %(mdproductiontype)s --mdappversion %(version)s --mdfiletype mc --mdappfamily art --mdruntype physics --mdgroupname %(group)s --tfilemdjsonname %(basename)s.root.json
#Point job_output towards a non-existent output as it isn't used for this stage
job_output.addoutput   = no_output.root
job_output_1.addoutput = %(basename)s.root
job_output_1.add_to_dataset  = %(outputdatasetname)s
#Tweak the postscripts as we don't have an artroot file to operate on
#DIAGNOSTICS
job_setup.postscript_2 = sbndpoms_tfilemetadata_extractor.sh %(basename)s.root
job_setup.postscript_3 = echo postscript_3 overriden to do nothing in anatree
# 02/2021 ETyley
# Add in caf stage. Metadata is handles by the `TFileMetadataSBND service which despite the name works for any file
[stage_caf]
global.basename       = caf
global.mdprojectstage = caf
global.fclfile        = cafmakerjob_%(experiment)s.fcl
submit.dataset        = %(dataset)s
executable.arg_3      =
executable.arg_4      =
#job_output.dest       = %(scratchoutdir)s
job_setup.prescript_2  = sbndpoms_metadata_injector.sh --inputfclname wrapper.fcl --mdfclname %(fclfile)s --mdprojectname %(mdprojectname)s --mdprojectstage %(mdprojectstage)s --mdprojectversion %(version)s --mdprojectsoftware sbndcode --mdproductionname %(mdproductionname)s --mdproductiontype %(mdproductiontype)s --mdappversion %(version)s --mdfiletype mc --mdappfamily art --mdruntype physics --mdgroupname %(group)s --tfilemdjsonname caf_%(basename)s.root.json --cafname caf_%(basename)s.root
job_setup.prescript_3  = cat wrapper.fcl
job_setup.postscript_2 = sbndpoms_tfilemetadata_extractor.sh caf_%(basename)s.root
job_setup.postscript_3 = echo postscript_4 overriden to do nothing in caf
job_output_2.add_to_dataset  = %(outputdatasetname)s
